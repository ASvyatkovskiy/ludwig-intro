{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification exercise\n",
    "\n",
    "This tutorial segment closely follows the [official webpage](https://uber.github.io/ludwig/examples/#text-classification).\n",
    "\n",
    "# Dataset\n",
    "\n",
    "We will be using the [Reuters dataset](https://martin-thoma.com/nlp-reuters/).\n",
    "Which is a benchmark dataset for document classification. It is a multi-class, multi-label (e.g. each document can belong to many classes) dataset, having 90 classes, 7769 training documents and 3019 testing documents. \n",
    "\n",
    "The training set has a vocabulary size of 35247. Even if you restrict it to words which appear at least 5 times and at most 12672 times in the training set, there are still 12017 words.\n",
    "\n",
    "## Classes and labels\n",
    "\n",
    "```bash\n",
    "                          nr of documents    mean number of\n",
    "       class name             train   test   words in train set\n",
    "     1: earn                : 2877    1087    104.4\n",
    "     2: acq                 : 1650     719    150.1\n",
    "     3: money-fx            :  538     179    219.0\n",
    "     4: grain               :  433     149    223.6\n",
    "     5: crude               :  389     189    247.3\n",
    "     6: trade               :  368     117    294.3\n",
    "     7: interest            :  347     131    198.0\n",
    "     8: wheat               :  212      71    225.6\n",
    "```     \n",
    "\n",
    "## Getting the Reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1188k  100 1188k    0     0   467k      0  0:00:02  0:00:02 --:--:--  468k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://boston.lti.cs.cmu.edu/classes/95-865-K/HW/HW2/reuters-allcats-6.zip\n",
    "#wget http://boston.lti.cs.cmu.edu/classes/95-865-K/HW/HW2/reuters-allcats-6.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  reuters-allcats-6.zip\n",
      "replace reuters-allcats.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip reuters-allcats-6.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check *reuters-allcats.csv* is in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ludwig',\n",
       " 'model_definition.yaml',\n",
       " 'reuters-allcats.csv',\n",
       " 'reuters-allcats.json',\n",
       " 'model_definition_time.yaml',\n",
       " 'Untitled.ipynb',\n",
       " 'reuters-allcats.hdf5',\n",
       " 'results',\n",
       " '.ipynb_checkpoints',\n",
       " 'environment.yaml',\n",
       " 'reuters-allcats-6.zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarizing ourselves with the data\n",
    "\n",
    "Before proceeding with the machine learning task, let us familiarize ourselves with the dataset.\n",
    "\n",
    "Print the column names in the CSV (header):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class,text\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 reuters-allcats.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names will be used as a config parameter for the model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neg-</td>\n",
       "      <td>2 BAHIA COCOA REVIEW     SALVADOR  Feb 26 - Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neg-</td>\n",
       "      <td>2 USX ltX DEBT DOWGRADED BY MOODYS     NEW YOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pos-earn</td>\n",
       "      <td>2 COBANCO INC ltCBCO YEAR NET     SANTA CRUZ  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pos-earn</td>\n",
       "      <td>2 BROWN-FORMAN INC ltBFD 4TH QTR NET     LOUIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neg-</td>\n",
       "      <td>2 HUGHES CAPITAL UNIT SIGNS PACT WITH BEAR STE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text\n",
       "0      Neg-  2 BAHIA COCOA REVIEW     SALVADOR  Feb 26 - Sh...\n",
       "1      Neg-  2 USX ltX DEBT DOWGRADED BY MOODYS     NEW YOR...\n",
       "2  Pos-earn  2 COBANCO INC ltCBCO YEAR NET     SANTA CRUZ  ...\n",
       "3  Pos-earn  2 BROWN-FORMAN INC ltBFD 4TH QTR NET     LOUIS...\n",
       "4      Neg-  2 HUGHES CAPITAL UNIT SIGNS PACT WITH BEAR STE..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reuters_raw = pd.read_csv(\"reuters-allcats.csv\")\n",
    "reuters_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     4079\n",
       "unique       7\n",
       "top       Neg-\n",
       "freq      1929\n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_raw['class'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neg-           1929\n",
       "Pos-earn       1280\n",
       "Pos-acq         790\n",
       "Pos-coffee       35\n",
       "Pos-gold         34\n",
       "Pos-housing       7\n",
       "Pos-heat          4\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_raw['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Ludwig model definition\n",
    "\n",
    "One can use Ludwig via CLI or Python API. Let us stick to the CLI for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model-definition.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile model-definition.yaml\n",
    "input_features:    # Described here https://uber.github.io/ludwig/user_guide/#input-features\n",
    "    -\n",
    "        name: text #name of the CSV column for feature\n",
    "        type: text\n",
    "        level: word #token vs character level granularity\n",
    "        encoder: parallel_cnn #type of NN\n",
    "\n",
    "output_features: #https://uber.github.io/ludwig/user_guide/#output-features\n",
    "    -\n",
    "        name: class #name of thge CSV column for label\n",
    "        type: category #categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "The entry point to the tool is the *ludwig* executable. We can check what argument it takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "usage: ludwig <command> [<args>]\n",
      "\n",
      "Available sub-commands:\n",
      "   experiment            Runs a full experiment training a model and testing it\n",
      "   train                 Trains a model\n",
      "   predict               Predicts using a pretrained model\n",
      "   visualize             Visualizes experimental results\n",
      "   collect_weights       Collects tensors containing a pretrained model weights\n",
      "   collect_activations   Collects tensors for each datapoint using a pretrained model\n",
      "\n",
      "ludwig cli runner\n",
      "\n",
      "positional arguments:\n",
      "  command     Subcommand to run\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "!ludwig --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "usage: ludwig experiment [options]\n",
      "\n",
      "This script trains and tests a model.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --output_directory OUTPUT_DIRECTORY\n",
      "                        directory that contains the results\n",
      "  --experiment_name EXPERIMENT_NAME\n",
      "                        experiment name\n",
      "  --model_name MODEL_NAME\n",
      "                        name for the model\n",
      "  --data_csv DATA_CSV   input data CSV file. If it has a split column, it will\n",
      "                        be used for splitting (0: train, 1: validation, 2:\n",
      "                        test), otherwise the dataset will be randomly split\n",
      "  --data_train_csv DATA_TRAIN_CSV\n",
      "                        input train data CSV file\n",
      "  --data_validation_csv DATA_VALIDATION_CSV\n",
      "                        input validation data CSV file\n",
      "  --data_test_csv DATA_TEST_CSV\n",
      "                        input test data CSV file\n",
      "  --data_hdf5 DATA_HDF5\n",
      "                        input data HDF5 file. It is an intermediate preprocess\n",
      "                        version of the input CSV created the first time a CSV\n",
      "                        file is used in the same directory with the same name\n",
      "                        and a hdf5 extension\n",
      "  --data_train_hdf5 DATA_TRAIN_HDF5\n",
      "                        input train data HDF5 file. It is an intermediate\n",
      "                        preprocess version of the input CSV created the first\n",
      "                        time a CSV file is used in the same directory with the\n",
      "                        same name and a hdf5 extension\n",
      "  --data_validation_hdf5 DATA_VALIDATION_HDF5\n",
      "                        input validation data HDF5 file. It is an intermediate\n",
      "                        preprocess version of the input CSV created the first\n",
      "                        time a CSV file is used in the same directory with the\n",
      "                        same name and a hdf5 extension\n",
      "  --data_test_hdf5 DATA_TEST_HDF5\n",
      "                        input test data HDF5 file. It is an intermediate\n",
      "                        preprocess version of the input CSV created the first\n",
      "                        time a CSV file is used in the same directory with the\n",
      "                        same name and a hdf5 extension\n",
      "  --metadata_json METADATA_JSON\n",
      "                        input metadata JSON file. It is an intermediate\n",
      "                        preprocess file containing the mappings of the input\n",
      "                        CSV created the first time a CSV file is used in the\n",
      "                        same directory with the same name and a json extension\n",
      "  -sspi, --skip_save_processed_input\n",
      "                        skips saving intermediate HDF5 and JSON files\n",
      "  -ssuo, --skip_save_unprocessed_output\n",
      "                        skips saving intermediate NPY output files\n",
      "  -md MODEL_DEFINITION, --model_definition MODEL_DEFINITION\n",
      "                        model definition\n",
      "  -mdf MODEL_DEFINITION_FILE, --model_definition_file MODEL_DEFINITION_FILE\n",
      "                        YAML file describing the model. Ignores\n",
      "                        --model_hyperparameters\n",
      "  -mlp MODEL_LOAD_PATH, --model_load_path MODEL_LOAD_PATH\n",
      "                        path of a pretrained model to load as initialization\n",
      "  -mrp MODEL_RESUME_PATH, --model_resume_path MODEL_RESUME_PATH\n",
      "                        path of a the model directory to resume training of\n",
      "  -ssm, --skip_save_model\n",
      "                        disables saving model weights and hyperparameters each\n",
      "                        time the model imrpoves. By default Ludwig saves model\n",
      "                        weights after each epoch the validation measure\n",
      "                        imrpvoes, but if the model is really big that can be\n",
      "                        time consuming if you do not want to keep the weights\n",
      "                        and just find out what performance can a model get\n",
      "                        with a set of hyperparameters, use this parameter to\n",
      "                        skip it,but the model will not be loadable later on.\n",
      "  -ssp, --skip_save_progress\n",
      "                        disables saving progress each epoch. By default Ludwig\n",
      "                        saves weights and stats after each epoch for enabling\n",
      "                        resuming of training, but if the model is really big\n",
      "                        that can be time consuming and will uses twice as much\n",
      "                        space, use this parameter to skip it, but training\n",
      "                        cannot be resumed later on.\n",
      "  -ssl, --skip_save_log\n",
      "                        disables saving TensorBoard logs. By default Ludwig\n",
      "                        saves logs for the TensorBoard, but if it is not\n",
      "                        needed turning it off can slightly increase the\n",
      "                        overall speed.\n",
      "  -rs RANDOM_SEED, --random_seed RANDOM_SEED\n",
      "                        a random seed that is going to be used anywhere there\n",
      "                        is a call to a random number generator: data\n",
      "                        splitting, parameter initialization and training set\n",
      "                        shuffling\n",
      "  -g GPUS [GPUS ...], --gpus GPUS [GPUS ...]\n",
      "                        list of GPUs to use\n",
      "  -gf GPU_FRACTION, --gpu_fraction GPU_FRACTION\n",
      "                        fraction of gpu memory to initialize the process with\n",
      "  -uh, --use_horovod    uses horovod for distributed training\n",
      "  -dbg, --debug         enables debugging mode\n",
      "  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n",
      "                        the level of logging to use\n"
     ]
    }
   ],
   "source": [
    "!ludwig experiment --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      " _         _        _      \n",
      "| |_  _ __| |_ __ _(_)__ _ \n",
      "| | || / _` \\ V  V / / _` |\n",
      "|_|\\_,_\\__,_|\\_/\\_/|_\\__, |\n",
      "                     |___/ \n",
      "ludwig v0.1.1 - Experiment\n",
      "\n",
      "/anaconda2/envs/myenv/lib/python3.6/site-packages/ludwig/experiment.py:178: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  model_definition = merge_with_defaults(yaml.load(def_file))\n",
      "Experiment name: experiment\n",
      "Model name: run\n",
      "Output path: results/experiment_run_13\n",
      "\n",
      "ludwig_version: '0.1.1'\n",
      "command: ('/anaconda2/envs/hello-ludwig/bin/ludwig experiment '\n",
      " '--data_csv=reuters-allcats.csv --model_definition_file=model-definition.yaml')\n",
      "dataset_type: 'generic'\n",
      "random_seed: 42\n",
      "input_data: 'reuters-allcats.csv'\n",
      "model_definition: {   'combiner': {'type': 'concat'},\n",
      "    'input_features': [   {   'encoder': 'parallel_cnn',\n",
      "                              'level': 'word',\n",
      "                              'name': 'text',\n",
      "                              'tied_weights': None,\n",
      "                              'type': 'text'}],\n",
      "    'output_features': [   {   'dependencies': [],\n",
      "                               'loss': {   'class_similarities_temperature': 0,\n",
      "                                           'class_weights': 1,\n",
      "                                           'confidence_penalty': 0,\n",
      "                                           'distortion': 1,\n",
      "                                           'labels_smoothing': 0,\n",
      "                                           'negative_samples': 0,\n",
      "                                           'robust_lambda': 0,\n",
      "                                           'sampler': None,\n",
      "                                           'type': 'softmax_cross_entropy',\n",
      "                                           'unique': False,\n",
      "                                           'weight': 1},\n",
      "                               'name': 'class',\n",
      "                               'reduce_dependencies': 'sum',\n",
      "                               'reduce_input': 'sum',\n",
      "                               'top_k': 3,\n",
      "                               'type': 'category'}],\n",
      "    'preprocessing': {   'bag': {   'fill_value': '',\n",
      "                                    'format': 'space',\n",
      "                                    'lowercase': False,\n",
      "                                    'missing_value_strategy': 'fill_with_const',\n",
      "                                    'most_common': 10000},\n",
      "                         'binary': {   'fill_value': 0,\n",
      "                                       'missing_value_strategy': 'fill_with_const'},\n",
      "                         'category': {   'fill_value': '<UNK>',\n",
      "                                         'lowercase': False,\n",
      "                                         'missing_value_strategy': 'fill_with_const',\n",
      "                                         'most_common': 10000},\n",
      "                         'force_split': False,\n",
      "                         'image': {   'in_memory': True,\n",
      "                                      'missing_value_strategy': 'backfill',\n",
      "                                      'resize_method': 'crop_or_pad'},\n",
      "                         'numerical': {   'fill_value': 0,\n",
      "                                          'missing_value_strategy': 'fill_with_const'},\n",
      "                         'sequence': {   'fill_value': '',\n",
      "                                         'format': 'space',\n",
      "                                         'lowercase': False,\n",
      "                                         'missing_value_strategy': 'fill_with_const',\n",
      "                                         'most_common': 20000,\n",
      "                                         'padding': 'right',\n",
      "                                         'padding_symbol': '<PAD>',\n",
      "                                         'sequence_length_limit': 256,\n",
      "                                         'unknown_symbol': '<UNK>'},\n",
      "                         'set': {   'fill_value': '',\n",
      "                                    'format': 'space',\n",
      "                                    'lowercase': False,\n",
      "                                    'missing_value_strategy': 'fill_with_const',\n",
      "                                    'most_common': 10000},\n",
      "                         'split_probabilities': (0.7, 0.1, 0.2),\n",
      "                         'stratify': None,\n",
      "                         'text': {   'char_format': 'characters',\n",
      "                                     'char_most_common': 70,\n",
      "                                     'char_sequence_length_limit': 1024,\n",
      "                                     'fill_value': '',\n",
      "                                     'lowercase': True,\n",
      "                                     'missing_value_strategy': 'fill_with_const',\n",
      "                                     'padding': 'right',\n",
      "                                     'padding_symbol': '<PAD>',\n",
      "                                     'unknown_symbol': '<UNK>',\n",
      "                                     'word_format': 'space_punct',\n",
      "                                     'word_most_common': 20000,\n",
      "                                     'word_sequence_length_limit': 256},\n",
      "                         'timeseries': {   'fill_value': '',\n",
      "                                           'format': 'space',\n",
      "                                           'missing_value_strategy': 'fill_with_const',\n",
      "                                           'padding': 'right',\n",
      "                                           'padding_value': 0,\n",
      "                                           'timeseries_length_limit': 256}},\n",
      "    'training': {   'batch_size': 128,\n",
      "                    'bucketing_field': None,\n",
      "                    'decay': False,\n",
      "                    'decay_rate': 0.96,\n",
      "                    'decay_steps': 10000,\n",
      "                    'dropout_rate': 0.0,\n",
      "                    'early_stop': 5,\n",
      "                    'epochs': 100,\n",
      "                    'eval_batch_size': 0,\n",
      "                    'gradient_clipping': None,\n",
      "                    'increase_batch_size_on_plateau': 0,\n",
      "                    'increase_batch_size_on_plateau_max': 512,\n",
      "                    'increase_batch_size_on_plateau_patience': 5,\n",
      "                    'increase_batch_size_on_plateau_rate': 2,\n",
      "                    'learning_rate': 0.001,\n",
      "                    'learning_rate_warmup_epochs': 5,\n",
      "                    'optimizer': {   'beta1': 0.9,\n",
      "                                     'beta2': 0.999,\n",
      "                                     'epsilon': 1e-08,\n",
      "                                     'type': 'adam'},\n",
      "                    'reduce_learning_rate_on_plateau': 0,\n",
      "                    'reduce_learning_rate_on_plateau_patience': 5,\n",
      "                    'reduce_learning_rate_on_plateau_rate': 0.5,\n",
      "                    'regularization_lambda': 0,\n",
      "                    'regularizer': 'l2',\n",
      "                    'staircase': False,\n",
      "                    'validation_field': 'combined',\n",
      "                    'validation_measure': 'loss'}}\n",
      "\n",
      "Found hdf5 and json with the same filename of the csv, using them instead\n",
      "Using full hdf5 and json\n",
      "Loading data from: reuters-allcats.hdf5\n",
      "Loading metadata from: reuters-allcats.json\n",
      "Training set: 2868\n",
      "Validation set: 389\n",
      "Test set: 822\n",
      "WARNING:tensorflow:From /anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "From /anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "From /anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "From /anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "From /anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "\n",
      "╒══════════╕\n",
      "│ TRAINING │\n",
      "╘══════════╛\n",
      "\n",
      "2019-04-18 21:55:56.685862: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "\n",
      "Epoch   1\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:34<00:00,  3.51s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:20<00:00,  1.30it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.44it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:05<00:00,  1.31it/s]\n",
      "Took 2m 4.5443s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.7366 │     0.8410 │      0.9829 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 1.3649 │     0.7661 │      0.9692 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.8174 │     0.7993 │      0.9781 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   2\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:32<00:00,  3.43s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:20<00:00,  1.33it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.44it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:05<00:00,  1.32it/s]\n",
      "Took 2m 1.3745s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.3406 │     0.9446 │      0.9840 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.7698 │     0.8843 │      0.9692 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.4850 │     0.8869 │      0.9781 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   3\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:37<00:00,  3.92s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:23<00:00,  1.18it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.39it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:05<00:00,  1.30it/s]\n",
      "Took 2m 9.9031s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.1557 │     0.9672 │      0.9930 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.4789 │     0.8869 │      0.9692 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.3739 │     0.8942 │      0.9842 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   4\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:32<00:00,  3.49s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:20<00:00,  1.33it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.44it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:05<00:00,  1.29it/s]\n",
      "Took 2m 2.4336s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.2020 │     0.9027 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.8259 │     0.7558 │      0.9897 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.6748 │     0.7920 │      0.9903 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 1 epoch ago\n",
      "\n",
      "\n",
      "Epoch   5\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:32<00:00,  3.43s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:20<00:00,  1.31it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.44it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:05<00:00,  1.31it/s]\n",
      "Took 2m 1.7900s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0390 │     0.9941 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.3771 │     0.8920 │      0.9923 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.2918 │     0.9100 │      0.9903 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   6\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:36<00:00,  3.55s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:21<00:00,  1.29it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.38it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:05<00:00,  1.31it/s]\n",
      "Took 2m 6.7069s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0208 │     0.9983 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.4931 │     0.8663 │      0.9949 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.3534 │     0.8942 │      0.9964 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 1 epoch ago\n",
      "\n",
      "\n",
      "Epoch   7\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:33<00:00,  3.43s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:20<00:00,  1.32it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.44it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:05<00:00,  1.32it/s]\n",
      "Took 2m 2.4681s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0108 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.2584 │     0.9152 │      0.9949 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.2024 │     0.9307 │      0.9976 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   8\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:32<00:00,  3.44s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:20<00:00,  1.33it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.44it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:05<00:00,  1.33it/s]\n",
      "Took 2m 1.5238s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0064 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.3235 │     0.9126 │      0.9794 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.2397 │     0.9282 │      0.9866 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 1 epoch ago\n",
      "\n",
      "\n",
      "Epoch   9\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:34<00:00,  3.97s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:23<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.43it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:06<00:00,  1.12it/s]\n",
      "Took 2m 7.3636s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0042 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.3252 │     0.9075 │      0.9871 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.2386 │     0.9270 │      0.9915 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 2 epochs ago\n",
      "\n",
      "\n",
      "Epoch  10\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:39<00:00,  3.61s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:24<00:00,  1.23it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:03<00:00,  1.25it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:06<00:00,  1.16it/s]\n",
      "Took 2m 14.2628s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0030 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.3575 │     0.8946 │      0.9846 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.2582 │     0.9246 │      0.9927 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 3 epochs ago\n",
      "\n",
      "\n",
      "Epoch  11\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:39<00:00,  3.82s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:21<00:00,  1.28it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:02<00:00,  1.39it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:06<00:00,  1.21it/s]\n",
      "Took 2m 10.5471s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0026 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.3789 │     0.8869 │      0.9846 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.2726 │     0.9221 │      0.9927 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 4 epochs ago\n",
      "\n",
      "\n",
      "Epoch  12\n",
      "Training: 100%|█████████████████████████████████| 23/23 [01:44<00:00,  3.76s/it]\n",
      "Evaluation train: 100%|█████████████████████████| 23/23 [00:22<00:00,  1.20it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:03<00:00,  1.27it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 7/7 [00:07<00:00,  1.07it/s]\n",
      "Took 2m 17.4728s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ class   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0023 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.3705 │     0.8920 │      0.9846 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.2654 │     0.9246 │      0.9927 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 5 epochs ago\n",
      "\n",
      "EARLY STOPPING due to lack of validation improvement, it has been 5 epochs since last validation accuracy improvement\n",
      "\n",
      "Best validation model epoch: 7\n",
      "Best validation model loss on validation set combined: 0.25841310704581844\n",
      "Best validation model loss on test set combined: 0.20242815934248504\n",
      "\n",
      "╒═════════╕\n",
      "│ PREDICT │\n",
      "╘═════════╛\n",
      "\n",
      "Evaluation: 100%|█████████████████████████████████| 7/7 [00:07<00:00,  1.05it/s]\n",
      "/Users/alexeys-laptop/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexeys-laptop/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexeys-laptop/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "===== class =====\n",
      "accuracy: 0.9245742092457421\n",
      "hits_at_k: 0.9927007299270073\n",
      "loss: 0.2654371226791048\n",
      "overall_stats: { 'avg_f1_score_macro': 0.5307573420760888,\n",
      "  'avg_f1_score_micro': 0.9245742092457421,\n",
      "  'avg_f1_score_weighted': 0.9154239455428493,\n",
      "  'avg_precision_macro': 0.6349726001511716,\n",
      "  'avg_precision_micro': 0.9245742092457421,\n",
      "  'avg_precision_weighted': 0.9245742092457421,\n",
      "  'avg_recall_macro': 0.5033592750075162,\n",
      "  'avg_recall_micro': 0.9245742092457421,\n",
      "  'avg_recall_weighted': 0.9245742092457421,\n",
      "  'kappa_score': 0.8781704046165394,\n",
      "  'overall_accuracy': 0.9245742092457421}\n",
      "per_class_stats: {<UNK>: {   'accuracy': 1.0,\n",
      "    'f1_score': 0,\n",
      "    'fall_out': 0.0,\n",
      "    'false_discovery_rate': 1.0,\n",
      "    'false_negative_rate': 1.0,\n",
      "    'false_negatives': 0,\n",
      "    'false_omission_rate': 0.0,\n",
      "    'false_positive_rate': 0.0,\n",
      "    'false_positives': 0,\n",
      "    'hit_rate': 0,\n",
      "    'informedness': 0.0,\n",
      "    'markedness': 0.0,\n",
      "    'matthews_correlation_coefficient': 0,\n",
      "    'miss_rate': 1.0,\n",
      "    'negative_predictive_value': 1.0,\n",
      "    'positive_predictive_value': 0,\n",
      "    'precision': 0,\n",
      "    'recall': 0,\n",
      "    'sensitivity': 0,\n",
      "    'specificity': 1.0,\n",
      "    'true_negative_rate': 1.0,\n",
      "    'true_negatives': 822,\n",
      "    'true_positive_rate': 0,\n",
      "    'true_positives': 0},\n",
      "  Neg-: {   'accuracy': 0.9306569343065694,\n",
      "    'f1_score': 0.932061978545888,\n",
      "    'fall_out': 0.018372703412073532,\n",
      "    'false_discovery_rate': 0.017587939698492483,\n",
      "    'false_negative_rate': 0.11337868480725621,\n",
      "    'false_negatives': 50,\n",
      "    'false_omission_rate': 0.11792452830188682,\n",
      "    'false_positive_rate': 0.018372703412073532,\n",
      "    'false_positives': 7,\n",
      "    'hit_rate': 0.8866213151927438,\n",
      "    'informedness': 0.8682486117806703,\n",
      "    'markedness': 0.8644875319996208,\n",
      "    'matthews_correlation_coefficient': 0.8663660309363291,\n",
      "    'miss_rate': 0.11337868480725621,\n",
      "    'negative_predictive_value': 0.8820754716981132,\n",
      "    'positive_predictive_value': 0.9824120603015075,\n",
      "    'precision': 0.9824120603015075,\n",
      "    'recall': 0.8866213151927438,\n",
      "    'sensitivity': 0.8866213151927438,\n",
      "    'specificity': 0.9816272965879265,\n",
      "    'true_negative_rate': 0.9816272965879265,\n",
      "    'true_negatives': 374,\n",
      "    'true_positive_rate': 0.8866213151927438,\n",
      "    'true_positives': 391},\n",
      "  Pos-earn: {   'accuracy': 0.9805352798053528,\n",
      "    'f1_score': 0.9673469387755103,\n",
      "    'fall_out': 0.022336769759450203,\n",
      "    'false_discovery_rate': 0.052000000000000046,\n",
      "    'false_negative_rate': 0.012499999999999956,\n",
      "    'false_negatives': 3,\n",
      "    'false_omission_rate': 0.005244755244755206,\n",
      "    'false_positive_rate': 0.022336769759450203,\n",
      "    'false_positives': 13,\n",
      "    'hit_rate': 0.9875,\n",
      "    'informedness': 0.9651632302405497,\n",
      "    'markedness': 0.9427552447552447,\n",
      "    'matthews_correlation_coefficient': 0.9538934412994945,\n",
      "    'miss_rate': 0.012499999999999956,\n",
      "    'negative_predictive_value': 0.9947552447552448,\n",
      "    'positive_predictive_value': 0.948,\n",
      "    'precision': 0.948,\n",
      "    'recall': 0.9875,\n",
      "    'sensitivity': 0.9875,\n",
      "    'specificity': 0.9776632302405498,\n",
      "    'true_negative_rate': 0.9776632302405498,\n",
      "    'true_negatives': 569,\n",
      "    'true_positive_rate': 0.9875,\n",
      "    'true_positives': 237},\n",
      "  Pos-acq: {   'accuracy': 0.9586374695863747,\n",
      "    'f1_score': 0.8851351351351352,\n",
      "    'fall_out': 0.03665689149560114,\n",
      "    'false_discovery_rate': 0.16025641025641024,\n",
      "    'false_negative_rate': 0.06428571428571428,\n",
      "    'false_negatives': 9,\n",
      "    'false_omission_rate': 0.013513513513513487,\n",
      "    'false_positive_rate': 0.03665689149560114,\n",
      "    'false_positives': 25,\n",
      "    'hit_rate': 0.9357142857142857,\n",
      "    'informedness': 0.8990573942186846,\n",
      "    'markedness': 0.8262300762300763,\n",
      "    'matthews_correlation_coefficient': 0.8618748513331372,\n",
      "    'miss_rate': 0.06428571428571428,\n",
      "    'negative_predictive_value': 0.9864864864864865,\n",
      "    'positive_predictive_value': 0.8397435897435898,\n",
      "    'precision': 0.8397435897435898,\n",
      "    'recall': 0.9357142857142857,\n",
      "    'sensitivity': 0.9357142857142857,\n",
      "    'specificity': 0.9633431085043989,\n",
      "    'true_negative_rate': 0.9633431085043989,\n",
      "    'true_negatives': 657,\n",
      "    'true_positive_rate': 0.9357142857142857,\n",
      "    'true_positives': 131},\n",
      "  Pos-coffee: {   'accuracy': 0.9841849148418491,\n",
      "    'f1_score': 0,\n",
      "    'fall_out': 0.015815085158150888,\n",
      "    'false_discovery_rate': 1.0,\n",
      "    'false_negative_rate': 1.0,\n",
      "    'false_negatives': 0,\n",
      "    'false_omission_rate': 0.0,\n",
      "    'false_positive_rate': 0.015815085158150888,\n",
      "    'false_positives': 13,\n",
      "    'hit_rate': 0,\n",
      "    'informedness': -0.015815085158150888,\n",
      "    'markedness': 0.0,\n",
      "    'matthews_correlation_coefficient': 0,\n",
      "    'miss_rate': 1.0,\n",
      "    'negative_predictive_value': 1.0,\n",
      "    'positive_predictive_value': 0.0,\n",
      "    'precision': 0.0,\n",
      "    'recall': 0,\n",
      "    'sensitivity': 0,\n",
      "    'specificity': 0.9841849148418491,\n",
      "    'true_negative_rate': 0.9841849148418491,\n",
      "    'true_negatives': 809,\n",
      "    'true_positive_rate': 0,\n",
      "    'true_positives': 0},\n",
      "  Pos-gold: {   'accuracy': 0.9963503649635036,\n",
      "    'f1_score': 0.4,\n",
      "    'fall_out': 0.0036540803897685548,\n",
      "    'false_discovery_rate': 0.75,\n",
      "    'false_negative_rate': 0.0,\n",
      "    'false_negatives': 0,\n",
      "    'false_omission_rate': 0.0,\n",
      "    'false_positive_rate': 0.0036540803897685548,\n",
      "    'false_positives': 3,\n",
      "    'hit_rate': 1.0,\n",
      "    'informedness': 0.9963459196102313,\n",
      "    'markedness': 0.25,\n",
      "    'matthews_correlation_coefficient': 0.49908564385539866,\n",
      "    'miss_rate': 0.0,\n",
      "    'negative_predictive_value': 1.0,\n",
      "    'positive_predictive_value': 0.25,\n",
      "    'precision': 0.25,\n",
      "    'recall': 1.0,\n",
      "    'sensitivity': 1.0,\n",
      "    'specificity': 0.9963459196102314,\n",
      "    'true_negative_rate': 0.9963459196102314,\n",
      "    'true_negatives': 818,\n",
      "    'true_positive_rate': 1.0,\n",
      "    'true_positives': 1},\n",
      "  Pos-housing: {   'accuracy': 1.0,\n",
      "    'f1_score': 0,\n",
      "    'fall_out': 0.0,\n",
      "    'false_discovery_rate': 1.0,\n",
      "    'false_negative_rate': 1.0,\n",
      "    'false_negatives': 0,\n",
      "    'false_omission_rate': 0.0,\n",
      "    'false_positive_rate': 0.0,\n",
      "    'false_positives': 0,\n",
      "    'hit_rate': 0,\n",
      "    'informedness': 0.0,\n",
      "    'markedness': 0.0,\n",
      "    'matthews_correlation_coefficient': 0,\n",
      "    'miss_rate': 1.0,\n",
      "    'negative_predictive_value': 1.0,\n",
      "    'positive_predictive_value': 0,\n",
      "    'precision': 0,\n",
      "    'recall': 0,\n",
      "    'sensitivity': 0,\n",
      "    'specificity': 1.0,\n",
      "    'true_negative_rate': 1.0,\n",
      "    'true_negatives': 822,\n",
      "    'true_positive_rate': 0,\n",
      "    'true_positives': 0},\n",
      "  Pos-heat: {   'accuracy': 0.9987834549878345,\n",
      "    'f1_score': 0,\n",
      "    'fall_out': 0.0012165450121655041,\n",
      "    'false_discovery_rate': 1.0,\n",
      "    'false_negative_rate': 1.0,\n",
      "    'false_negatives': 0,\n",
      "    'false_omission_rate': 0.0,\n",
      "    'false_positive_rate': 0.0012165450121655041,\n",
      "    'false_positives': 1,\n",
      "    'hit_rate': 0,\n",
      "    'informedness': -0.0012165450121655041,\n",
      "    'markedness': 0.0,\n",
      "    'matthews_correlation_coefficient': 0,\n",
      "    'miss_rate': 1.0,\n",
      "    'negative_predictive_value': 1.0,\n",
      "    'positive_predictive_value': 0.0,\n",
      "    'precision': 0.0,\n",
      "    'recall': 0,\n",
      "    'sensitivity': 0,\n",
      "    'specificity': 0.9987834549878345,\n",
      "    'true_negative_rate': 0.9987834549878345,\n",
      "    'true_negatives': 821,\n",
      "    'true_positive_rate': 0,\n",
      "    'true_positives': 0}}\n",
      "\n",
      "Finished: experiment_run\n",
      "Saved to: results/experiment_run_13\n"
     ]
    }
   ],
   "source": [
    "!ludwig experiment --data_csv=reuters-allcats.csv --model_definition_file=model-definition.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class_probabilities.csv',\n",
       " 'class_probability.csv',\n",
       " 'prediction_statistics.json',\n",
       " 'class_probabilities.npy',\n",
       " 'class_probability.npy',\n",
       " 'description.json',\n",
       " 'model',\n",
       " 'class_predictions.csv',\n",
       " 'training_statistics.json',\n",
       " 'class_predictions.npy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"./results/experiment_run_13/training_statistics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\n",
      "For more information, please see:\r\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n",
      "  * https://github.com/tensorflow/addons\r\n",
      "If you depend on functionality not listed there, please file an issue.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!ludwig visualize --visualization learning_curves --training_statistics results/experiment_run_13/training_statistics.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running more experiments\n",
    "\n",
    "Change neural network to RNN or LSTM, repeat experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hello, Ludwig",
   "language": "python",
   "name": "hello-ludwig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
